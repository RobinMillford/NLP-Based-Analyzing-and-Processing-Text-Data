# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jxpOY5RxaIT3d8Eb54HPFR8MfLhpgJPC
"""

#importing library
import pandas as pd
import requests
from bs4 import BeautifulSoup
import numpy as np
from google.colab import files
import nltk
import string
nltk.download('punkt')

df = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/Blackcoffer/Input.xlsx')
df.head()

df.drop('URL_ID',axis=1,inplace=True)

url_id = 1
data_list = []

for i in range(len(df)):
    j = df.iloc[i].values

    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'}
    page = requests.get(j[0], headers=headers)  # loading text in url
    soup = BeautifulSoup(page.content, 'html.parser')  # parsing url text

    # Extracting content text
    content = soup.find('div', {'class': 'td-post-content'})
    if content:
        content = content.get_text().replace('\xa0', ' ').replace('\n', ' ')
    else:
        content = ''

    # Extracting title of website
    title = soup.find('h1')
    if title:
        title = title.get_text().replace('\n', '').replace('/', '')
    else:
        title = ''

    # Merging title and content text
    text = title + '. ' + content

    # Append data to the list
    data_list.append([url_id, j[0], title, content])

    url_id += 1

# Create DataFrame from the data list
columns = ['URL_ID', 'URL', 'ARTICLE_TITLE', 'ARTICLE_CONTENT']
df_output = pd.DataFrame(data_list, columns=columns)

# Save DataFrame to CSV file
df_output.to_csv('output.csv', index=False)

txt = pd.read_csv("/content/output.csv")

txt.info()

txt.head()

txt = txt.drop(columns=['URL_ID', 'URL'])

txt=txt.astype(str)

import re

# Create an empty list to store the sentences
sentences = []

# Loop through each row in the DataFrame
for index, row in txt.iterrows():
    # Split the 'ARTICLE_CONTENT' text into sentences using regex
    content = row['ARTICLE_CONTENT']
    # Split the sentences without keeping the punctuation marks
    sentences.extend(re.split(r'[.!?]', content))

# Create a new DataFrame with the sentences
sentences_df = pd.DataFrame(sentences, columns=['SENTENCE'])

# Define a function to remove '.' from the text
def remove_dot(x):
    return x.replace('.', '')

# Apply the 'remove_dot' function to the 'SENTENCE' column of DataFrame 'sentences_df'
sentences_df['SENTENCE'] = sentences_df['SENTENCE'].apply(remove_dot)

# Replacing empty spaces with NaN
c = sentences_df.replace(r'^\s*$', np.nan, regex=True)

# Dropping rows with NaN values
c = c.dropna()

# Resetting the index of the DataFrame
c.reset_index(drop=True, inplace=True)

c.head()

#importing nltk library and stopwords
import nltk
import string

punc=[punc for punc in string.punctuation]

punc

#importing stop words files that are provided
StopWords_Auditor = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Blackcoffer/StopWords/StopWords_Auditor.txt", header=None, encoding="utf-8")
stopwords_currencies = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Blackcoffer/StopWords/StopWords_Currencies.txt", sep='|', header=None, names=['CURRENCY', 'COUNTRY'], encoding='ISO-8859-1')
StopWords_DatesandNumbers = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Blackcoffer/StopWords/StopWords_DatesandNumbers.txt", header=None, encoding="utf-8")
StopWords_Generic = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Blackcoffer/StopWords/StopWords_Generic.txt", header=None, encoding="utf-8")
StopWords_GenericLong = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Blackcoffer/StopWords/StopWords_GenericLong.txt", header=None, encoding="utf-8")
StopWords_Geographic = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Blackcoffer/StopWords/StopWords_Geographic.txt", header=None, encoding="utf-8")
StopWords_Names = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Blackcoffer/StopWords/StopWords_Names.txt", header=None, encoding="utf-8")

import unicodedata

def text_process(text):
    # Replace special Unicode characters with regular characters
    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')

    # Remove punctuation characters
    nopunc = [char for char in text if char not in punc or char not in [':', ',', '(', ')', 'â€™', '?']]
    nopunc = ''.join(nopunc)

    # Remove stop words from different categories
    stop_words = StopWords_Auditor + stopwords_currencies + StopWords_DatesandNumbers + StopWords_Generic + StopWords_GenericLong + StopWords_Geographic + StopWords_Names
    filtered_words = [word for word in nopunc.split() if word.lower() not in stop_words]

    return ' '.join(filtered_words)

c['SENTENCE'] = c['SENTENCE'].apply(text_process)

c.head()

c.info()

positive = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Blackcoffer/MasterDictionary/positive-words.txt", header=None, encoding="ISO-8859-1")
negative = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Blackcoffer/MasterDictionary/negative-words.txt", header=None, encoding="ISO-8859-1")

positive.columns=['abc']
negative.columns=['abc']
positive['abc']=positive['abc'].astype(str)
negative['abc']=negative['abc'].astype(str)

#positive and negative dictionary without stopwords
positive['abc']=positive['abc'].apply(text_process)
negative['abc']=negative['abc'].apply(text_process)

#positive list
length=positive.shape[0]
post=[]
for i in range(0,length):
   nopunc =[char for char in positive.iloc[i] if char not in string.punctuation or char != '+']
   nopunc=''.join(nopunc)

   post.append(nopunc)

#negative list
length=negative.shape[0]
neg=[]
for i in range(0,length):
  nopunc =[char for char in negative.iloc[i] if char not in string.punctuation or char != '+']
  nopunc=''.join(nopunc)
  neg.append(nopunc)

from nltk.tokenize import word_tokenize

txt_list = []
for i in range(len(c)):
    txt = c['SENTENCE'].iloc[i]  # Replace 'abc' with the correct column name in your DataFrame
    txt_list.append(word_tokenize(txt))

print(txt_list)

len(txt_list)

positive_score_list = []
for sentence_tokens in txt_list:
    positive_score = 0
    for token in sentence_tokens:
        if token.lower() in post:
            positive_score += 1
    positive_score_list.append(positive_score)

c['POSITIVE SCORE'] = positive_score_list
print(c)

negative_score_list = []
for sentence_tokens in txt_list:
    negative_score = 0
    for token in sentence_tokens:
        if token.lower() in neg:
            negative_score += 1
    negative_score_list.append(negative_score)

c['NEGATIVE SCORE'] = negative_score_list
print(c)

polarity_score_list = []
for i in range(len(c)):
    positive_score = c['POSITIVE SCORE'].iloc[i]
    negative_score = c['NEGATIVE SCORE'].iloc[i]
    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)
    polarity_score_list.append(polarity_score)

c['POLARITY SCORE'] = polarity_score_list
print(c)

subjectivity_score_list = []
for i in range(len(c)):
    positive_score = c['POSITIVE SCORE'].iloc[i]
    negative_score = c['NEGATIVE SCORE'].iloc[i]
    total_words = len(txt_list[i])
    subjectivity_score = (positive_score + negative_score) / (total_words + 0.000001)
    subjectivity_score_list.append(subjectivity_score)

c['SUBJECTIVITY SCORE'] = subjectivity_score_list
print(c)

# Calculate AVG SENTENCE LENGTH
avg_sentence_length_list = []

for i in range(len(c)):
    sentence = c['SENTENCE'].iloc[i]
    words = word_tokenize(sentence)
    avg_sentence_length = len(words)
    avg_sentence_length_list.append(avg_sentence_length)

c['AVG SENTENCE LENGTH'] = avg_sentence_length_list

vowels = ['a', 'e', 'i', 'o', 'u']
complex_Word_Count_list = []

for i in range(len(c)):
    sentence = c['SENTENCE'].iloc[i]
    sentence_tokens = word_tokenize(sentence)

    count = 0
    complex_Word_Count = 0

    if len(sentence_tokens) > 0:
        for word in sentence_tokens:
            if word.lower()[-2:] in ['es', 'ed']:
                count += 0
            else:
                for char in word:
                    if char.lower() in vowels:
                        count += 1
            if count > 2:
                complex_Word_Count += 1
            count = 0

        percentage_complex_words = (complex_Word_Count / len(sentence_tokens)) * 100
    else:
        percentage_complex_words = 0

    complex_Word_Count_list.append(percentage_complex_words)

c['PERCENTAGE OF COMPLEX WORDS'] = complex_Word_Count_list
print(c)

Fog_Index_list = []

for i in range(len(c)):
    avg_sentence_length = len(c['SENTENCE'].iloc[i])
    percentage_complex_words = c['PERCENTAGE OF COMPLEX WORDS'].iloc[i]

    Fog_Index = 0.4 * (avg_sentence_length + percentage_complex_words)
    Fog_Index_list.append(Fog_Index)

c['FOG INDEX'] = Fog_Index_list
print(c)

# Calculate AVG NUMBER OF WORDS PER SENTENCE
avg_no_of_words_per_sentence_list = []

for i in range(len(c)):
    sentence = c['SENTENCE'].iloc[i]
    words = word_tokenize(sentence)
    avg_no_of_words_per_sentence = len(words)
    avg_no_of_words_per_sentence_list.append(avg_no_of_words_per_sentence)

c['AVG NUMBER OF WORDS PER SENTENCE'] = avg_no_of_words_per_sentence_list

print(c)

# Define a function to calculate the complex word count for a given sentence
def calculate_complex_word_count(sentence_tokens):
    vowels = ['a', 'e', 'i', 'o', 'u']
    count = 0
    complex_word_count = 0

    for i in sentence_tokens:
        x = re.compile('[es|ed]$')
        if x.match(i.lower()):
            count += 0
        else:
            for j in i:
                if j.lower() in vowels:
                    count += 1
        if count > 2:
            complex_word_count += 1
        count = 0

    return complex_word_count


# Calculate COMPLEX WORD COUNT for each row
complex_word_count_list = []

for sentence_tokens in txt_list:
    complex_word_count = calculate_complex_word_count(sentence_tokens)
    complex_word_count_list.append(complex_word_count)

# Add COMPLEX WORD COUNT to the DataFrame
c['COMPLEX WORD COUNT'] = complex_word_count_list

print(c)

# Define a function to calculate the word count for a given sentence
def calculate_word_count(sentence_tokens):
    return len(sentence_tokens)

# Calculate WORD COUNT for each row
word_count_list = []

for sentence_tokens in txt_list:
    word_count = calculate_word_count(sentence_tokens)
    word_count_list.append(word_count)

# Add WORD COUNT to the DataFrame
c['WORD COUNT'] = word_count_list

print(c)

vowels = ['a', 'e', 'i', 'o', 'u']
import re

# Create a list to store the syllable count for each row
syllable_per_word_list = []

# Calculate syllable count per word for each row
for sentence_tokens in txt_list:
    count = 0
    for i in sentence_tokens:
        x = re.compile('[es|ed]$')
        if x.match(i.lower()):
            count += 0
        else:
            for j in i:
                if j.lower() in vowels:
                    count += 1
    syllable_per_word_list.append(count)

# Add SYLLABLE PER WORD to the DataFrame 'c'
c['SYLLABLE PER WORD'] = syllable_per_word_list

print(c)

# Define a function to calculate the personal pronouns count in a given sentence
def calculate_personal_pronouns(sentence_tokens):
    pronouns = ['i', 'we', 'my', 'mine', 'ours', 'us', 'our', 'ours']
    return sum(1 for token in sentence_tokens if token.lower() in pronouns)

# Calculate PERSONAL PRONOUNS for each row
personal_pronouns_list = []

for sentence_tokens in txt_list:
    personal_pronouns = calculate_personal_pronouns(sentence_tokens)
    personal_pronouns_list.append(personal_pronouns)

# Add PERSONAL PRONOUNS to the DataFrame
c['PERSONAL PRONOUNS'] = personal_pronouns_list

print(c)

# Define a function to calculate the average word length in a given sentence
def calculate_avg_word_length(sentence_tokens):
    if len(sentence_tokens) > 0:
        total_length = sum(len(token) for token in sentence_tokens)
        return total_length / len(sentence_tokens)
    else:
        return 0

# Calculate AVG WORD LENGTH for each row
avg_word_length_list = []

for sentence_tokens in txt_list:
    avg_word_length = calculate_avg_word_length(sentence_tokens)
    avg_word_length_list.append(avg_word_length)

# Add AVG WORD LENGTH to the DataFrame
c['AVG WORD LENGTH'] = avg_word_length_list

print(c)

output = pd.DataFrame(c)

output.head()

output.to_csv('output.csv', index=False)

from google.colab import files

files.download('output.csv')